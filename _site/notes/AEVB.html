<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Auto-Encoding Variational Bayes &amp; Application to Biomedical Segmentation | Jade Sea</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Auto-Encoding Variational Bayes &amp; Application to Biomedical Segmentation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Derivation Notes" />
<meta property="og:description" content="Derivation Notes" />
<link rel="canonical" href="http://localhost:4000/notes/AEVB.html" />
<meta property="og:url" content="http://localhost:4000/notes/AEVB.html" />
<meta property="og:site_name" content="Jade Sea" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-07-30T09:09:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Auto-Encoding Variational Bayes &amp; Application to Biomedical Segmentation" />
<script type="application/ld+json">
{"url":"http://localhost:4000/notes/AEVB.html","headline":"Auto-Encoding Variational Bayes &amp; Application to Biomedical Segmentation","dateModified":"2018-07-30T09:09:00+02:00","datePublished":"2018-07-30T09:09:00+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/notes/AEVB.html"},"description":"Derivation Notes","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

  </head>
  <body>

    <header class="page-header" role="banner">
      <h1 class="project-name">Auto-Encoding Variational Bayes & Application to Biomedical Segmentation</h1>
      <h2 class="project-tagline">Derivation Notes</h2>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <p><strong>Journal Club Notes by Jianxiao</strong></p>

<p>This note includes general derivations<sup id="fnref:fn1"><a href="#fn:fn1" class="footnote">1</a></sup> and specific derivations for segmentation application<sup id="fnref:fn2"><a href="#fn:fn2" class="footnote">2</a></sup>.</p>

<h2 id="1-generic-auto-encoding-variational-bayes-aevb">1. Generic Auto-Encoding Variational Bayes (AEVB)</h2>
<h3 id="11-problem-scenario">1.1. Problem scenario</h3>
<p><img src="https://lh3.googleusercontent.com/1ch85gktT6CXHwGuMpGPeIn9a_Dfgc_4ZsOydCtUlybuuiwTi4fJmnYwM6q1FQQ6i8H66JzgyeW2" alt="figure 1" height="200px" width="200px" /></p>

<p><em>Figure 1. Directed graphical model considered</em></p>

<p>Consider a model consisting of a dataset $X = { x^{(i)} }^N_{i=1}$ and its associated latent variables $z$, shown in Figure 1. Values of variable $z$ are generated from a prior distribution $p_\theta(z)$; values of variable $x$ are generated from the conditional distribution  $p_\theta(x|z)$.</p>

<p>Inference based on this model can be intractable. Specifically, the marginal likelihood $p_\theta(x) = \int p_\theta(z)p_\theta(x|z) \mathop{dz}$ is intractable as it requires integrating over the latent variable $z$. As a result, the posteriror density $p_\theta(z|x) = \frac{p_\theta(x|z)p_\theta(z)}{p_\theta(x)}$ is also intractable as it involves evaluating $p_\theta(x)$ first.</p>

<p>The Auto-Encoding Variational Bayes (AEVB) approach is proposed for inference of marginal likelihood $p_\theta(x)$ and posterior density $p_\theta(z|x)$, the approximate of which also help to estimate the parameters $\theta$ using ML or MAP.</p>

<h3 id="12-variational-lower-bound">1.2. Variational lower bound</h3>

<p>We start by introducing an approximation $q_\phi(z|x)$ to the intractable true posterior $p_\theta(z|x)$. We can consider the KL divergence between these two distribution:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
KL [q_\phi(z|x) || p_\theta(z|x)] &= \mathbb{E}_q [\log q_\phi(z|x) - \log p_\theta(z|x)] \\
& = \mathbb{E}_q [\log q_\phi(z|x) - \log \frac{p_\theta(x,z)}{p_\theta(x)}] \\
& = \mathbb{E}_q [\log q_\phi(z|x) - \log p_\theta(x,z) + \log p_\theta(x)] \\
\end{aligned} %]]></script>

<p>Rearranging the terms gives:</p>

<script type="math/tex; mode=display">\mathbb{E}_q [\log p_\theta(x)] = KL [q_\phi(z|x) || p_\theta(z|x)] - \mathbb{E}_q [\log q_\phi(z|x) - \log p_\theta(x,z)]</script>

<p>Since the first term is the non-negative KL divergence, the second term is the lower bound of the log marginal likelihood $\log p_\theta(x)$, also called the variational lower bound.  The variational lower bound can be written in two forms:</p>

<p>1) The joint-contrastive form</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\mathcal{L}(\theta,\phi;x) &= \mathbb{E}_q  [-\log q_\phi(z|x) + \log p_\theta(x,z)]
\end{aligned} %]]></script>

<p>2)  The prior-contrastive form</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\mathcal{L}(\theta,\phi;x) &= \mathbb{E}_q  [-\log q_\phi(z|x) + \log p_\theta(x,z)] \\
& = \mathbb{E}_q  [-\log q_\phi(z|x) + \log (p_\theta(x|z)p_\theta(z))] \\
& = \mathbb{E}_q  [-\log q_\phi(z|x) + \log p_\theta(x|z) + \log p_\theta(z)] \\
& = \mathbb{E}_q [\log p_\theta(x|z)] - \mathbb{E}_q [\log q_\phi(z|x) - \log p_\theta(z)] \\
& = \mathbb{E}_q [\log p_\theta(x|z)] -  KL [q_\phi(z|x) || p_\theta(z)]
\end{aligned} %]]></script>

<p>The prior-contrastive form can be related to auto-encoders: the KL term acts as a regularisation term while the expectation term is the reconstruction error ($z$ is sampled from $x$ by $g(\epsilon,x)$; $x$ is then sampled from $z$ by $p_\theta(x|z)$).</p>

<h3 id="13-stochastic-gradient-variational-bayes-sgvb-estimator">1.3. Stochastic Gradient Variational Bayes (SGVB) estimator</h3>

<p>In order to optimise the $\mathcal{L}(\theta,\phi;x)$, we need to differentiate it w.r.t. $\theta$ and $\phi$. However, differentiating w.r.t. $\phi$ is problematic as the usual Monte Carlo gradient estimator exhibits high variance<sup id="fnref:fn3"><a href="#fn:fn3" class="footnote">3</a></sup>. The usual Monte Carlo method estimates the <script type="math/tex">\mathbb{E}_q[\cdot]</script> terms by sampling $z$ from $q_\phi(z|x)$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\nabla_\phi \mathbb{E}_q [f(z)] &= \mathbb{E}_q [f(z) \nabla_q \log q_\phi(z)] \\
& \approx \frac{1}{L} \sum^L_{l=1} f(z) \nabla_q \log q_\phi(z[l])
\end{aligned} %]]></script>

<p>Instead, we use a reparametrisation trick so that the Monte Carlo estimate becomes differentiable. We reparametrise $z$ as:</p>

<script type="math/tex; mode=display">\begin{aligned}
z = g_\phi(\epsilon,x) \quad with \quad \epsilon \sim p(\epsilon)
\end{aligned}</script>

<p>where the transformation $g_\phi(\cdot)$ and auxiliary noise $\epsilon$ are chosen based on the form of $q_\phi(z|x)$.</p>

<p><img src="https://lh3.googleusercontent.com/BK0ZkSWopD6KfK37JU-38qFU0Ycm3nJ0TWSg3iDfjmY1y2xvqPwY876JCsZ5_Yy33MRTJon8SxjH" alt="figure 2" /></p>

<p><em>Figure 2 How reparametrisation trick changes network structure</em></p>

<p>Essentially, we are assuming that sampling $z$ from $q_\phi(z|x)$ is equivalent to sampling $\epsilon$ from $p(\epsilon)$ (Figure 2). As a result, we can rewrite integrals involving $q_\phi(z|x)$ in forms involving $\epsilon$ instead:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\int q_\phi(z|x) f(z) \mathop{dz} &= \int p(\epsilon)  f(g_\phi(\epsilon,x)) \mathop{d\epsilon} \\
& \approx \frac{1}{L} \sum^L_{l=1} f(g_\phi(x,\epsilon[l]))
\end{aligned} %]]></script>

<p>We can then compute the SGVB estimator for both forms of variational lower bound:</p>

<p>1)  The joint-contrastive form</p>

<script type="math/tex; mode=display">\begin{aligned}
\widetilde{\mathcal{L}}(\theta,\phi;x) =  \frac{1}{L} \sum^L_{l=1} (-\log q_\phi(z[l]|x) + \log p_\theta(x,z[l]))
\end{aligned}</script>

<p>2)  The prior-contrastive form</p>

<script type="math/tex; mode=display">\begin{aligned}
\widetilde{\mathcal{L}}(\theta,\phi;x) =\frac{1}{L} \sum^L_{l=1} (\log p_\theta(x|z[l])) -  KL [q_\phi(z|x) || p_\theta(z)]
\end{aligned}</script>

<p>The prior-contrastive form typically has less variance, but relies on the KL term having closed form to work. In particular, if both the true prior and the approximate posterior are Gaussian, the KL term can be easily computed.</p>

<h2 id="2-application-to-biomedical-segmentation">2. Application to biomedical segmentation</h2>
<h3 id="21-problem-scenario">2.1. Problem scenario</h3>

<p>CNN-based Biomedical segmentation usually requires large set of annotated data and does not take into account of anatomical knowledge. In the Dalca et al. paper<sup id="fnref:fn2:1"><a href="#fn:fn2" class="footnote">2</a></sup>, an two-step AEVB approach is proposed to carry out unsupervised segmentation learning, with anatomical prior incorporated.</p>

<p>Consider a model consisting of image data $x$, its segmentation map $s$, and the latent variable $z$, which represents the underlying shape embedding (Figure 3).</p>

<p><img src="https://lh3.googleusercontent.com/X7srV2YDKAxDnkIx5mP5lxBV0O92KPlCHHjtWoYLBj0bY8r6TQwdyT1jYrpfyHYFA6bDA6VT1yLE" alt="figure 3" /></p>

<p><em>Figure 3 Generative model for the segmentation problem</em></p>

<p>The model generates values based on the following prior and likelihood densities:</p>

<p>1)   the prior probability of the latent variable $z$ is modeled by a standard normal distribution</p>

<script type="math/tex; mode=display">p(z) = \mathcal{N}(z;0,\mathrm{I})</script>

<p>2)   the labels $s$ are drawn from a categorical distribution given $z$</p>

<script type="math/tex; mode=display">p_{\theta_{s|z}}(s|z) = \prod_j f_{j,s[j]} (z|\theta_{s|z})</script>

<p>where $f_{j,m}(\cdot; \theta_{s|z})$ is the probability of label $m$ at voxel $j$</p>

<p>3)  the voxel intensity values $x$ are drawn from normal distributions with diagonal covariance matrix</p>

<script type="math/tex; mode=display">p_{\theta_{x|s}}(x|s) = \prod_j \prod_m \mathcal{N} (x[j]; \mu_m, \sigma_m^2)^{\delta(s[j]=m)}</script>

<p>where $\delta(s[j]=m) = 1$ if $s[j]=m$ and $0$ otherwise.</p>

<p>We aim to learn segmentation from an image using maximum a posteriori (MAP) estimation:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\hat{s} &= \mathop{\mathrm{arg\,max}}_{s} \log p_\theta(s|x) \\
& =\mathop{\mathrm{arg\,max}}_{s} \log \frac{p_\theta(s, x)}{p(x)} \\
& =\mathop{\mathrm{arg\,max}}_{s} \log p_\theta(s, x)
\end{aligned} %]]></script>

<p>However, $p_\theta(s,x)$ is intractable as it requires integrating over $z$:</p>

<script type="math/tex; mode=display">p_\theta(s, x) = p_\theta(x|s)p(s) = p_\theta(x|s) \int p_\theta(s|z)p(z) \mathop{dz}</script>

<p>Similarly, $p_\theta(z|x,s)$ is also intractable, which mean EM algorithm does not work in this case.</p>

<p>Instead, we will optimise $\log p_\theta(x)$ and $\log p_\theta(s)$ by optimising their variational lower bounds respectively. Note that we cannot optimise the two probability distributions together because we do not have paired sets of $x$ and $s$ in unsupervised learning setting. Nevertheless, we want to learn mappings between the latent space and both the image space and the label space, so that we can apply anatomical knowledged learnt from label-latent mapping to new images.</p>

<h3 id="22-sgvb-estimator-derivations">2.2. SGVB estimator derivations</h3>
<h4 id="221-learning-anatomical-prior">2.2.1. Learning anatomical prior</h4>

<p>Using the AEVB framework, we approximate the true posterior $p_\theta(z|s)$ with $q_\phi(z|s)$. $q_\phi(z|s)$ is modeled as a normal distribution with diagnoal covariance matrix:</p>

<script type="math/tex; mode=display">q_\phi(z|s) = \mathcal{N} (z; \mu_{z|s}, \sigma_{z|s}^2)</script>

<p>The respective variational lower bound is:</p>

<script type="math/tex; mode=display">\mathcal{L}(\theta,\phi;s) = \mathbb{E}_q [\log p_\theta(s|z)] -  KL [q_\phi(z|s) || p(z)]</script>

<p>Using the reparametrisation trick, we can estimate the expectation term as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\mathbb{E}_q [\log p_\theta(s|z)] &\approx \frac{1}{L} \sum^L_{l=1} \log p_\theta(s|z[l]) \\
& = \frac{1}{L} \sum_l \log (\prod_j f_{j,s[j]} (z[l]|\theta_{s|z})) \\
& = \frac{1}{L} \sum_l \sum_j \log f_{j,s[j]} (z[l]|\theta_{s|z})
\end{aligned} %]]></script>

<p>Since both the true prior $p(z)$ and the approximate posterior $q_\phi(z|s)$ are gaussian, the KL term has a closed form (see Appendix A for the proof):</p>

<script type="math/tex; mode=display">KL [q_\phi(z|s) || p(z)] = \frac{1}{2} \sum_j (1 + \log (\sigma_{z|s}[j]^2) - \mu_{z|s}[j]^2 - \sigma_{z|s}[j]^2)</script>

<h4 id="222-unsupervised-learning">2.2.2. Unsupervised learning</h4>

<p>Similar to the previous section, we approximate the true posterior  $p_\theta(z|x)$ with $q_\phi(z|x)$. $q_\phi(z|x)$ is modeled as a normal distribution with diagnoal covariance matrix:</p>

<script type="math/tex; mode=display">q_\phi(z|x) = \mathcal{N} (z; \mu_{z|x}, \sigma_{z|x}^2)</script>

<p>The respective variational lower bound is:</p>

<script type="math/tex; mode=display">\mathcal{L}(\theta,\phi;x) = \mathbb{E}_q [\log p_\theta(x|z)] -  KL [q_\phi(z|x) || p(z)]</script>

<p>Again, the KL term has a closed-form solution:</p>

<script type="math/tex; mode=display">KL [q_\phi(z|x) || p(z)] = \frac{1}{2} \sum_j (1 + \log (\sigma_{z|x}[j]^2) - \mu_{z|x}[j]^2 - \sigma_{z|x}[j]^2)</script>

<p>The expectation term, on the other hand, is intractable as we cannot compute $p_\theta(x|z)$. Instead, we will use a lower bound of the term derived based on Jensen’s inequality (more details in Appendix B):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\mathbb{E}_q [\log p_\theta(x|z)] &= \mathbb{E}_q [\log \int_s p_\theta(x,s|z) \mathop{ds}] \\
& = \mathbb{E}_q [\log \int_s p_\theta(s|z) p_\theta(x|s) \mathop{ds}] \\
& \ge \mathbb{E} [\int p_\theta(s|z) \log p_\theta(x|s) \mathop{ds}] \\
& = \int_s \prod_j f_{j,s[j]} (z|\theta_{s|z}) \log \prod_j \prod_m \mathcal{N} (x[j]; \mu_m, \sigma_m^2)^{\delta(s[j]=m)} \mathop{ds} \\
& = \int_s \prod_j f_{j,s[j]} (z|\theta_{s|z}) \sum_j \sum_m \delta(s[j]=m) \log \mathcal{N} (x[j]; \mu_m, \sigma_m^2) \mathop{ds} \\
& = \sum_j \sum_m \int_s \delta(s[j]=m) \prod_j f_{j,s[j]} (z|\theta_{s|z}) \mathop{ds} \log \mathcal{N} (x[j]; \mu_m, \sigma_m^2) \\
& = \sum_j \sum_m f_{j,m} (z|\theta_{s|z}) \log \mathcal{N} (x[j]; \mu_m, \sigma_m^2) \\
& \sim \sum_j \sum_m f_{j,m} (z|\theta_{s|z}) \frac{-(x[j]-\mu_m)^2}{2\sigma_m^2} \\
\end{aligned} %]]></script>

<p>We can then estimate the expectation term, sampling $z$ using the reparametrisation trick:</p>

<script type="math/tex; mode=display">\mathbb{E} [\int p_\theta(s|z) \log p_\theta(x|s) \mathop{ds}] = \frac{1}{L} \sum_l \sum_j \sum_m f_{j,m} (z[l]|\theta_{s|z}) \frac{-(x[j]-\mu_m)^2}{2\sigma_m^2}</script>

<h3 id="23-network-architecture">2.3. Network architecture</h3>

<p>Two CNNs are used, one for learning the anatomical prior and one for unsupervised learning of segmentation from image.</p>

<p>Figure 4 shows the auto-encoder architecture for learning anatomical prior. This network learns the mapping between the latent space and the label space, using only segmentation maps.</p>

<p><img src="https://lh3.googleusercontent.com/ahkaKoqSOR0GFLkz3_xODtrvQUp9JyKCjNUlwu5gSnj8JlcXafGvZZiNJvxBnGJcubWB5eyf1dgb" alt="figure 4" /></p>

<p><em>Figure 4 Auto-encoder to learn anatomical prior</em></p>

<p>Given an input segmentation map $s$, the encoder outputs the parameter of the posterior $q_\phi(z|s)$, $\mu_{z|s}$ and $\sigma_{z|s}$. The latent variable $z$ is then sampled using reparametrisation trick, where $z \sim g(\epsilon,s) = \mu_{z|s} + \sigma_{z|s} \epsilon$. The transformation $g(\cdot)$ approximates normal distribution by adding a scaled noise (or variance) to the mean. Finally, $s$ is reconstructed by the decoder with $p_\theta(s|z)$, represented by the top output layer in Figure 4.</p>

<p>An additional prior is added in actual implementation, represented by the bottom output layer in Figure 4. This location prior $p_{loc}(s)$ is computed as the voxel-wise frequency of labels in the prior training dataset. By multiplying the prior $p_{loc}(s)$ with the reconstructed segmentation map from $p_\theta(s|z)$ (adding the logarithms in actual implementation), the output segmentation is obtained.</p>

<p>The parameters of $q_\phi(z|s)$ and $p_\theta(s|z)$, as well as the output segmentation labels $s[j]$, are then used to estimate and optimise the SGVB lower bound.</p>

<p>Figure 5 shows the architecture for unsupervised learning of segmentation from input images. The decoder part of this network is copied from the prior-learning network from Figure 4, which helps to incorporate the prior anatomical knowledge learnt.</p>

<p><img src="https://lh3.googleusercontent.com/ItyP2IhSon5MJ4E_Hc4a9Gc85JjX1HEJ0cbsJFmJtxHYpzuQx_FXWdKz-ArUexKPLM0GcB_sVGlj" alt="figure 5" /></p>

<p><em>Figure 5 Architecture for unsupervised learning</em></p>

<p>The image encoder is first trained in a similar fashion as the prior auto-encoder; the pre-trained weights are used for initialisation during actual training.</p>

<p>Given an input scan $x$, the image encoder outputs the parameter of the posterior $q_\phi(z|x)$, $\mu_{z|x}$ and $\sigma_{z|x}$. The latent variable $z$ is then sampled using reparametrisation trick, where $z \sim g(\epsilon,x) = \mu_{z|x} + \sigma_{z|x} \epsilon$. Then, the already trained prior decoder outputs $s$, which is used to generate the reconstructed $x$ with $p_\theta(x|s)$.</p>

<p>The parameters of $q_\phi(z|x)$ and $p_\theta(x|s)$, as well as the reconstructed image $x$, are then used to estimate and optimise the SGVB lower bound.</p>

<h3 id="24-results">2.4. Results</h3>

<p>The networks are tested on two datasets (Figure 6):</p>

<p>1)  T1w scan dataset: more than 14,000 T1-weighted MRI scans from ADNI, ABIDE, GSP, etc., all resampled to 256x256x256 (1mm isotropic) and cropped to 160x192x224 to remove entirely-background voxels.</p>

<p>2)  T2-FLAIR scan dataset: 3800 T2-FLAIR scans from ADNI (5mm slice spacing), linearly registered to T1w images (using ANTs). No ground truth.</p>

<p><img src="https://lh3.googleusercontent.com/sEmtOUA3wpT3-D5q5yh3SNmA-uKhxSsVdJpQDr6StSfz9UkOdlpDSuaD6tyZTyUuP4jdnIWDFqMf" alt="figure 6" /></p>

<p><em>Figure 6 Example T1w and T2-FLAIR images</em></p>

<p>A prior training set is composed using 5000 T1w images with ground truth segmentation maps generated by FreeSurfer (with manual correction and QC). This set is used to train the prior auto-encoder. The rest of the T1w images are split into trianing, validation and test sets for unsupervised learning. In the T2 scan case, subjects common to both datasets are excluded from the prior training set.</p>

<p>Figure 7 shows segmentation results from 3 subjects in the T1w scan dataset, in comparison to their respective ground truth. Figure 8 shows one subject’s segmentation results from the T2-FLAIR scan dataset. The estimated segmentation boundaries are generally aligned with the ground truth boudnaries, but much smoother, lacking of fine details.</p>

<p><img src="https://lh3.googleusercontent.com/SE46WYx9Un1fEJxoh65SZ3-h94imVxaQtBBp3Oh3j33_-WmkDlf9fc3DFhQPOdYgp6rOQcAeAuv6" alt="figure 7" /></p>

<p><em>Figure 7 Example segmentation results for T1w scan dataset</em></p>

<p><img src="https://lh3.googleusercontent.com/e1mMzIoxUV4xdH-TH0Y2Kg8LRDz1CsAgYUvnIeJB1gXGMEWcMTEwSrLEeEO0nbM4STqbhDUbaS5y" alt="figure 8" /></p>

<p><em>Figure 8 Example segmentation results for T2-FLAIR scan dataset</em></p>

<h2 id="appendix">Appendix</h2>

<h3 id="a-closed-form-solution-for-kl-divergence">A. Closed-form solution for KL divergence</h3>

<p>When two distributions are both gaussian, the KL divergence distance between them has a closed form. I will start with the proof for the generic case, and then show the solutions for a special case.</p>

<p>Consider two mutivariate normal distribution of dimension $k$, $p(x) \sim \mathcal{N} (\mu_1, \Sigma_1)$ and $q(x) \sim \mathcal{N} (\mu_2, \Sigma_2)$. There is no constraint on the form of covariance matrix. Since the KL divergence is defined as $KL(p(x)||q(x)) = \mathbb{E}_p [\log p(x) - \log q(x)]$ for continuous distributions, we start by expressing log likelihood for the two distributions.</p>

<p>For $p(x)$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
\log p(x) &= \log (\frac{1}{(2\pi)^{\frac{k}{2}} |\Sigma_1|^{\frac{1}{2}}} \exp^{-\frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)}) \\
& = -\frac{k}{2} \log 2\pi - \frac{1}{2} \log |\Sigma_1|  - \frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)
\end{aligned} %]]></script>

<p>where <script type="math/tex">\vert \cdot \vert</script> is the matrix determinant.</p>

<p>Similarly for q(x):</p>

<script type="math/tex; mode=display">\log q(x) = -\frac{k}{2} \log 2\pi - \frac{1}{2} \log |\Sigma_2|  - \frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)</script>

<p>Then we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
KL(p(x)||q(x)) &= \mathbb{E}_p [\log p(x) - \log q(x)] \\
& = \mathbb{E}_p [( -\frac{k}{2} \log 2\pi - \frac{1}{2} \log |\Sigma_1|  - \frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)) \\ 
& \quad  - (-\frac{k}{2} \log 2\pi - \frac{1}{2} \log |\Sigma_2|  - \frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2))] \\
& = \mathbb{E}_p [\frac{1}{2} \log \frac{|\Sigma_2|}{|\Sigma_1|}  - \frac{1}{2} (x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1) + \frac{1}{2} (x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)] \\
& = \frac{1}{2} \log \frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} \mathrm{Tr} [(x-\mu_1)^T \Sigma_1^{-1} (x-\mu_1)] + \frac{1}{2} \mathbb{E}_p [(x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)] \\
& = \frac{1}{2} \log \frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} \mathrm{Tr} (\Sigma_1 \Sigma_1^{-1}) + \frac{1}{2} \mathbb{E}_p [(x-\mu_2)^T \Sigma_2^{-1} (x-\mu_2)] \\
& = \frac{1}{2} \log \frac{|\Sigma_2|}{|\Sigma_1|} - \frac{1}{2} \mathrm{Tr} (\mathrm{I}_k) + \frac{1}{2} \mathbb{E}_p [(x-\mu_1+\mu_1-\mu_2)^T \Sigma_2^{-1} (x-\mu_1+\mu_1-\mu_2)] \\
& = \frac{1}{2} \log \frac{|\Sigma_2|}{|\Sigma_1|} - \frac{k}{2} +  \frac{1}{2} \mathbb{E}_p [(x-\mu_1)^T\Sigma_2^{-1}(x-\mu_1) \\
& \quad + 2(x-\mu_1)^T \Sigma_2^{-1} (\mu_1-\mu_2) +  (\mu_1-\mu_2)^T \Sigma_2^{-1} (\mu_1-\mu_2)] \\
& = \frac{1}{2} \log \frac{|\Sigma_2|}{|\Sigma_1|} - \frac{k}{2} + \frac{1}{2} \mathrm{Tr} (\Sigma_1 \Sigma_2^{-1}) + 0 +  \frac{1}{2}(\mu_1-\mu_2)^T \Sigma_2^{-1} (\mu_1-\mu_2) \\
& = \frac{1}{2} [\log \frac{|\Sigma_2|}{|\Sigma_1|} - k + \mathrm{Tr} (\Sigma_1 \Sigma_2^{-1}) + (\mu_1-\mu_2)^T \Sigma_2^{-1} (\mu_1-\mu_2)]
\end{aligned} %]]></script>

<p>Some tricks used are:</p>

<p>1)  trace trick: $\mathbb{E}[x] = \mathbb{E}[\mathrm{Tr}(x)]$ if x is scalar</p>

<p>2)  $\mathbb{E}_p [ (x-\mu_1)^T \Sigma_2^{-1} (\mu_1-\mu_2)]$: this is equal to 0. The expression is essentially the expectation of a scaled and rotated version of $(x-\mu_1)$, which are still straight lines passing through the origin.</p>

<p>Now, consider a special case, where $p(x) \sim \mathcal{N} (\mu_1, \sigma_1^2)$ and $q(x) \sim \mathcal{N} (0, \mathrm{I})$. In this case, we will be able to simplify the solution even further:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
KL(p(x)||q(x)) &= \frac{1}{2} [\log \frac{|\Sigma_2|}{|\Sigma_1|} - k + \mathrm{Tr} (\Sigma_1 \Sigma_2^{-1}) + (\mu_1-\mu_2)^T \Sigma_2^{-1} (\mu_1-\mu_2)] \\
& = \frac{1}{2} [\log \frac{|\mathrm{I}|}{\sigma_1^2} - k + \mathrm{Tr} (\sigma_1^2 \mathrm{I}^{-1}) + (\mu_1-0)^T \mathrm{I}^{-1} (\mu_1-0)] \\
& = \frac{1}{2} (-\log \sigma_1^2 - k + \sigma_1^2 + \mu_1^2)
\end{aligned} %]]></script>

<p>This is the solution used in section 2.2.</p>

<h3 id="b-jensens-inequality">B. Jensen’s inequality</h3>

<p>Jensen’s inequality states that the secant line of a convex function lies above the graph of the function. The concept is intuitive.</p>

<p><img src="https://lh3.googleusercontent.com/QshruYudFhIRlWl34iPqYev9dtB9yC5YVNbuUSUt-Z1MJyZTwrIqYPc-7Dqa-haD59lVsR4SrcEA" alt="jensen" /></p>

<p>Mathematically, for any convex function $f$, Jensen’s inequality is:</p>

<script type="math/tex; mode=display">f(\lambda x_1 + (1-\lambda) x_2) \le \lambda f(x_1) + (1-\lambda) f(x_2)</script>

<p>Conversely, for any concave function $g$, the sign is simply flipped:</p>

<script type="math/tex; mode=display">g(\lambda x_1 + (1-\lambda) x_2) \ge \lambda g(x_1) + (1-\lambda) g(x_2)</script>

<p>When probability density functions are involved (e.g. when computing expectation), Jensen’s inequality is:</p>

<script type="math/tex; mode=display">f(\int_p p(x) g(x) \mathop{dx}) \le \int_p p(x) f(g(x)) \mathop{dx}</script>

<p>where $f$ is the convex function, $g$ is any real-valued measurable function, and $p(x)$ is the probability density function (i.e. $\int_p p(x) \mathop{dx} = 1$).</p>

<p>In section 2.2.2, $\log$ is a concave function, and $p_\theta(s|z)$ is the probability density function. Therefore:</p>

<script type="math/tex; mode=display">\log (\int_s p_\theta(s|z) g(x) \mathop{ds}) \ge \int_s p_\theta(s|z) \log(g(x)) \mathop{ds}</script>

<div class="footnotes">
  <ol>
    <li id="fn:fn1">
      <p>Diederik P Kingma and Max Welling. “Auto-Encoding Variational Bayes”. In: ArXiv e-prints (2013). arXiv: 1312.6114 <a href="#fnref:fn1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:fn2">
      <p>Adrian V Dalca, John Guttag, and Mert R Sabuncu. “Anatomical Pri- ors in Convolutional Networks for Unsupervised Biomedical Segmenta- tion”. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, pp. 9290–9299. <a href="#fnref:fn2" class="reversefootnote">&#8617;</a> <a href="#fnref:fn2:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:fn3">
      <p>David M Blei, Michael I Jordan, and John W Paisley. “Variational Bayesian Inference with Stochastic Search”. In: Proceedings of the 29th International Conference on Machine Learning (ICML-12). 2012, pp. 1367– 1374. <a href="#fnref:fn3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>


      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
