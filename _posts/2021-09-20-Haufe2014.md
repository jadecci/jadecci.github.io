---
layout: default
title:  "Interpreting backward models (Haufe et al. 2014)"
description: "Derivation Notes"
date:   2021-09-20 15:09:00 +0800
categories: notes
---

This set of notes include explanation and derivations for the backward model interpretation approach from Haufe et al. 2014[^fn1].

## 1. Forward Models & Backward Models

To start, we define forward and backward models with respect to the same data. Assume we have $N$ samples (or subjects), each with $M$ channels (or features); also, assume we have $K$ latent factors (or target variables). Our data matrix would be $\bm{x}(n) = [x_1(n), ..., x_M(n)]^T \in \mathbb{R}^{M \times N}$, while the target matrix would be $\bm{y}(n) = [y_1(n), ..., y_K(n)]^T \in \mathbb{R}^{K \times N}$.

Forward models, also called generative models, represent the observed data as functions of latent variables. A linear forward model, in our case, would be $\bm{x}(n) = \bm{A}\bm{y}(n) + \epsilon(n)$, where the activation pattern $\bm{A} = [\bm{a_1}, ..., \bm{a_K}] \in \mathbb{R}^{M \times K}$ and the noise vector $\epsilon(n) \in \mathbb{R}^M$. The parameters of this forward model ($\bm{A}$) are interpretable directly, i.e. the magnitudes of the parameters can be used to assess how strongly the latent variables are related to the data.

Backward models, also called discriminative models, represent the target variables as functions of observed data. In many scenarios, backward models need to be employed as forward models may be difficult to formulate or simply intractable. However, parameters in backward models cannot be interpreted simply by looking at their magnitudes (see the original paper for examples). In our case, a linear backward model would be $\bm{y}(n) = \bm{W}^T\bm{x}(n)$, where the transformation matrix $\bm{W} \in \mathbb{R}^{M \times K}$.

## 2. Contructing Activation Patterns for Backward Models

In order to interpret backward model parameters, we can obtain an corresponding activation pattern matrix $\bm{A}$ based on $\bm{W}$. In the special case where $K = M$, we could simply compute $\bm{A} = \bm{W}^{-1}$. In more general cases, where $K \le M$, we need to solve for $\bm{A}$ satisfying the linear forward model $\bm{x}(n) = \bm{A}\bm{y}(n) + \epsilon(n)$.

We will make 3 assumptions (without loss of generality):

1. $\mathbb{E}[\bm{x}(n)]_n = \mathbb{E}[\bm{y}(n)]_n = \mathbb{E}[\epsilon(n)]_n = 0$, i.e. their covariance matrices are given by $\Sigma_x = \mathbb{E}[\bm{x}(n)\bm{x}(n)^T]_n$, etc.
2. $\bm{y}(n)$ are linearly independent ($\bm{W}$ has full rank), i.e. $\text{rank}(\bm{W}) = K$
3. $\epsilon(n)$ and $\bm{y}(n)$ are uncorrelated, i.e. $\mathbb{E}[\epsilon(n)\bm{y}(n)^T]_n = 0$

This leads us to **Theorem 1** which states that the corresponding forward model for our backward model is unique and its parameters are obtained by $\bm{A} = \Sigma_{\bm{x}}\bm{W}\Sigma_{\bm{y}}^{-1}$.

### 2.1. Proof of existence of a corresponding forward model

Assumptions 1 and 2 can be applied in general to properly processed data. Due to assumption 2, we know that the covariance matrix $\Sigma_{\bm{y}}$ is invertible; hence, $\bm{A}$ is well-defined. 

Now, we can show that assumption 3 is true when the forward model $\bm{x}(n) = \bm{A}\bm{y}(n) + \epsilon(n)$ and the activation pattern $\bm{A} = \Sigma_{\bm{x}}\bm{W}\Sigma_{\bm{y}}^{-1}$ are concerned. Specifically, these two formulations lead to

$$
\epsilon(n) = \bm{x}(n) - \bm{A} \bm{y}(n) = \bm{x}(n) - \Sigma_{\bm{x}} \bm{W} \Sigma_{\bm{y}}^{-1} \bm{y}(n)
$$

This in turn leads to

$$
\begin{aligned}
\mathbb{E} [\epsilon(n) \bm{y}(n)^T]_n & = \mathbb{E} [\bm{x}(n) \bm{y}(n)^T] - \mathbb{E} [\Sigma_{\bm{x}} \bm{W} \Sigma_{\bm{y}}^{-1} \bm{y}(n) \bm{y}(n)^T] \\
& = \mathbb{E} [\bm{x}(n) \bm{y}(n)^T] - \mathbb{E} [\Sigma_{\bm{x}} \bm{W}] \quad \text{(assumption 1)} \\
& = \mathbb{E} [\bm{x}(n) \bm{y}(n)^T] - \mathbb{E} [\bm{x}(n) \bm{x}(n)^T \bm{W}] \quad \text{(assumption 1)} \\
& = \mathbb{E} [\bm{x}(n) \bm{y}(n)^T] - \mathbb{E} [\bm{x}(n) \bm{y}(n)^T] \quad \text{(backward model)} \\
& = 0
\end{aligned}
$$

### 2.2. Proof of Theorem 1

Assuming the corresponding forward model exists, we can start proving **Theorem 1** by combining the formulation of the forward and backward model:

$$
\bm{y}(n) = \bm{W}^T \bm{x}(n) = \bm{W}^T (\bm{A} \bm{y}(n) + \epsilon(n)) = \bm{W}^T \bm{A} \bm{y}(n) + \bm{W}^T \epsilon(n)
$$

Then, we multiply both sides of the equation by $\bm{y}(n)^T$ and take the expected values over samples:

$$
\begin{aligned}
\mathbb{E} [\bm{y}(n) \bm{y}(n)^T] & = \mathbb{E} [\bm{W}^T \bm{A} \bm{y}(n) \bm{y}(n)^T] + \mathbb{E} [\bm{W}^T \epsilon(n) \bm{y}(n)^T] \\
& = \bm{W}^T \bm{A} \mathbb{E} [\bm{y}(n) \bm{y}(n)^T] + \bm{W}^T \mathbb{E} [\epsilon(n) \bm{y}(n)^T] \quad \text {($\bm{W}$ and $\bm{A}$ persist across samples) } \\
& = \bm{W}^T \bm{A} \mathbb{E} [\bm{y}(n) \bm{y}(n)^T] \quad \text {(assumption 3) } \\
\end{aligned}
$$

Since $\bm{y}(n)$ are linearly independent (assumption 2), $\mathbb{E} [\bm{y}(n) \bm{y}(n)^T]$ should have full rank. For the equation above to be true, it follows then that $\bm{W}^T \bm{A} = \bm{I}$.

Similarly, we can insert the backward model equation into the forward model equation:

$$
\bm{x}(n) = \bm{A} \bm{y}(n) + \epsilon(n) = \bm{A} \bm{W}^T \bm{x}(n) + \epsilon(n) \\
\text{or} \\
\epsilon(n) = \bm{x}(n) - (\bm{A} \bm{y}(n) + \epsilon(n)) = \bm{x}(n) - \bm{A} \bm{W}^T \bm{x}(n) = (\bm{I} - \bm{A} \bm{W}^T) \bm{x}(n)
$$

Then, we can show that

$$
\begin{aligned}
\bm{W}^T \mathbb{E} [\epsilon(n) \epsilon(n)] & = \bm{W}^T \mathbb{E} [ (\bm{I} - \bm{A} \bm{W}^T) \bm{x}(n) \epsilon(n)] \\
& = \bm{W}^T (\bm{I} - \bm{A} \bm{W}^T) \mathbb{E} [ \bm{x}(n) \epsilon(n)] \\
& = (\bm{W}^T - \bm{W}^T \bm{A} \bm{W}^T) \mathbb{E} [ \bm{x}(n) \epsilon(n)] \\
& = (\bm{W}^T - \bm{W}^T) \mathbb{E} [ \bm{x}(n) \epsilon(n)] \quad \text{since $\bm{W}^T \bm{A} = \bm{I}$} \\
& = 0
\end{aligned}
$$

Finally, let's see if we can obtain $\bm{A}$:

$$
\begin{aligned}
\Sigma_x \bm{W} \Sigma_{\bm{y}}^{-1} & = \mathbb{E} [\bm{x}(n) \bm{x}(n)^T] \bm{W} \Sigma_{\bm{y}}^{-1} \\
& = \mathbb{E} [(\bm{A} \bm{y}(n) + \epsilon(n)) (\bm{A} \bm{y}(n) + \epsilon(n))^T] \bm{W} \Sigma_{\bm{y}}^{-1} \quad \text {(forward model)} \\
& = (\bm{A} \mathbb{E} [ \bm{y}(n) \bm{A}^T \bm{y}(n)^T] + \bm{A} \mathbb{E} [\bm{y}(n) \epsilon(n)^T] + \mathbb{E} [ \epsilon(n) \bm{A}^T \bm{y}(n)^T] + \mathbb{E} [\epsilon(n) \epsilon(n)^T]) \bm{W} \Sigma_{\bm{y}}^{-1} \\
& = (\bm{A} \Sigma_{\bm{y}} \bm{A}^T  + \bm{A} \mathbb{E} [\bm{y}(n) \epsilon(n)^T] +\mathbb{E} [ \epsilon(n) \bm{A}^T \bm{y}(n)^T] + \Sigma_{\epsilon}) \bm{W} \Sigma_{\bm{y}}^{-1} \quad \text {(assumption 1)} \\
& = (\bm{A} \Sigma_{\bm{y}} \bm{A}^T + 0 + 0 + \Sigma_{\epsilon}) \bm{W} \Sigma_{\bm{y}}^{-1} \quad \text {(assumption 3)} \\
& = \bm{A} \Sigma_{\bm{y}} \bm{A}^T \bm{W} \Sigma_{\bm{y}}^{-1} + \Sigma_{\epsilon} \bm{W} \Sigma_{\bm{y}}^{-1}\\
& = \bm{A} + \Sigma_{\epsilon} \bm{W} \Sigma_{\bm{y}}^{-1} \quad \text{since $\bm{W}^T \bm{A} = \bm{I}$} \\
& = \bm{A} + 0 \quad \text {since $\bm{W}^T \mathbb{E} [\epsilon(n) \epsilon(n)] = \bm{W}^T \Sigma_{\epsilon} =  0$} \\
& = \bm{A}
\end{aligned}
$$




[^fn1]: Haufe S, Meinecke F, Goergen K, Dahne S, Haynes JD, Blankertz B, & Biessmann F. 2014. “On the Interpretation of Weight Vectors of Linear Models in Multivariate Neuroimaging”. *NeuroImage, 87*, 96-110.


